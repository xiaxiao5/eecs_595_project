{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will generate the benchmark data for the task \"Video-Language Misalignment Detection\".\n",
    "\n",
    "The task takes input a pair of video and narration in natural language. Then for each semantic group in the narration, the method should output if it aligns with the video or no.\n",
    "\n",
    "Variance\n",
    "- control alignment granularity with the semantic group granularity\n",
    "\n",
    "Challenging Cases\n",
    "- An object is in the video but not in the way as described in the narration. E.g., <a video of washing cucumber while a carrot is nearby, \"wash carrot\">\n",
    "\n",
    "Excepted Behavior\n",
    "- current method are performing bad on this benchmark which means this benchmark fill a gap in exiting evluation\n",
    "- Polysemy. E.g., \"nut_(tool)\" vs. \"nut_(food)\". \"turn_(off/on,switch)\"\n",
    "\n",
    "Others\n",
    "- potential method: a VL model (both ego-centric and exo-centric) trained with such fine-grained semantic sup-signal can be forced to learn faster and more capable of distinguishing fine-grained semantic difference. Accordingly, could improve performance for all VL tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load egoclip_srl.csv and browse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import itertools\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_720681/2730051815.py:2: DtypeWarning: Columns (20,25,28,30,31,32,33,34,35,36,37,38) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  egoclip_srl = pd.read_csv('../dataset/ego4d/annotation/egoclip_srl.csv')\n"
     ]
    }
   ],
   "source": [
    "# naive output of SRL method w/o post processing\n",
    "egoclip_srl = pd.read_csv('../dataset/ego4d/annotation/egoclip_srl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egoclip_srl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egoclip_srl.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egoclip_srl.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of clips have only have rols of 'ARG0', 'V', 'ARG1'. Suprisingly, 'ARG1' only has 80+% non-NaN values. We think the 20-% NaN are mostly false detecting by "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_counts = egoclip_srl.isna().sum()\n",
    "nan_ratios = nan_counts / len(egoclip_srl)\n",
    "\n",
    "nan_df = pd.concat([nan_counts, nan_ratios], axis=1)\n",
    "nan_df.columns = ['nan_counts', 'nan_ratios']\n",
    "nan_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns with too many NaNs. Then, drop rows >= 1 NaNs. The new dataframe would not have rows with NaN in V or ARG1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.8 # meaning for each column, >threshold% of rows should have non-NaN values\n",
    "clean_srl_df = egoclip_srl.dropna(axis=1, thresh=len(egoclip_srl)*threshold).dropna(axis=0)\n",
    "clean_srl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_srl_df['clip_uid'] = clean_srl_df.apply(lambda row: f\"{row['video_uid']}_{row['clip_start']}_{row['clip_end']}\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_srl_df.drop(columns=[\"index\"], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_srl_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_srl_df = clean_srl_df[[\"clip_uid\"] + list(clean_srl_df.columns[:-1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_srl_df[\"tag_verb\"] = clean_srl_df[\"tag_verb\"].apply(ast.literal_eval)\n",
    "clean_srl_df[\"tag_noun\"] = clean_srl_df[\"tag_noun\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_srl_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## benchmark\n",
    "For each group, k clips with the same group and k clips with different V and k clips with different ARG1, and k clips with different V and ARG1.\n",
    "\n",
    "The granularity of group depends on taxonomy. For example, \"cut cucumber\", \"dice courgette\" and \"slice fruit\" can be all in one group (divide fruit) or splited into 2 (\"cut cucumber\" and \"dice courgette\" vs. \"slice fruit\") or 3 groups.\n",
    "\n",
    "> TODO: visualize all narrations where closer semantic meaning will be close. Our job is to quantitatively measure model ability over different granularity of clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verb taxonomy: canonical verb -> verbs\n",
    "# noun taxonomy: canonical noun -> nouns\n",
    "# canonical narrations: canonical narration -> index of clips having the same canonical narration, ... different V, different ARG1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verb taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load taxonomy\n",
    "taxonomy_verb_path = '/z/dat/Ego4D/raw/v2/annotations/narration_verb_taxonomy.csv'\n",
    "taxonomy_verb = pd.read_csv(taxonomy_verb_path)\n",
    "taxonomy_verb['group'] = taxonomy_verb['group'].apply(ast.literal_eval)\n",
    "taxonomy_verb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make canonical_verb (will be the expression in canonical narration latter)\n",
    "taxonomy_verb[\"canonical_verb\"] = taxonomy_verb[\"label\"].str.split(\"_\\(\").apply(lambda x: x[0])\n",
    "display(taxonomy_verb.head())\n",
    "display(taxonomy_verb.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "noun taxonomy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomy_noun_path = \"/z/dat/Ego4D/raw/v2/annotations/narration_noun_taxonomy.csv\"\n",
    "taxonomy_noun = pd.read_csv(taxonomy_noun_path)\n",
    "taxonomy_noun['group'] = taxonomy_noun['group'].apply(ast.literal_eval)\n",
    "taxonomy_noun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomy_noun[\"canonical_noun\"] = taxonomy_noun[\"label\"].str.split(\"_\\(\").apply(lambda x: x[0])\n",
    "display(taxonomy_noun.head())\n",
    "display(taxonomy_noun.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There are duplicated canonical_noun but it's fine. That's because the clustering/grouping process will be based on the \"group\" column which has different values for differnt rows. THe canonical_noun is served as the word in narration so it is good to be natural instead of distinguishable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taxonomy_noun[\"canonical_noun\"].value_counts()\n",
    "taxonomy_noun[taxonomy_noun[\"canonical_noun\"].isin([\"nut\", \"bat\", \"pot\", \"chip\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the table of canonical narration.\n",
    "- assign V and ARG1 of each row of clip\n",
    "- extract all valid canonical narrations (V + ARG1) from all clips. At the same time, log which clips belongs to which row of canonical narrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the cross product of canonical verbs and canonical nouns\n",
    "canonical_verbs = taxonomy_verb[\"canonical_verb\"]\n",
    "canonical_nouns = taxonomy_noun[\"canonical_noun\"]\n",
    "cross_product = list(itertools.product(canonical_verbs, canonical_nouns))\n",
    "\n",
    "# Create a dataframe from the cross product\n",
    "canonical_narrations_df = pd.DataFrame(cross_product, columns=[\"canonical_verb\", \"canonical_noun\"])\n",
    "\n",
    "# Add columns representing the index of the verb and noun in the taxonomy dataframes\n",
    "canonical_narrations_df[\"verb_index\"] = [elem for elem in range(len(canonical_verbs)) for _ in range(len(canonical_nouns))]\n",
    "canonical_narrations_df[\"noun_index\"] = list(range(len(canonical_nouns))) * len(canonical_verbs)\n",
    "\n",
    "# Display the dataframe\n",
    "canonical_narrations_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# substantiate the index of canonical verbs and nouns are correct\n",
    "canonical_narrations_df['og_canonical_verb'] = canonical_narrations_df['verb_index'].apply(lambda x: taxonomy_verb.loc[x, 'canonical_verb'])\n",
    "sum(canonical_narrations_df[\"canonical_verb\"] != canonical_narrations_df[\"og_canonical_verb\"]) # should be 0. I.e., the value from cross-product and accessed by index are the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the semantic roles labeled for each clip narration. Next, for each role (e.g., V or ARG1), we put it into the semantic groups we want it to be based on the taxonomy we defined. (This is where we control the distinguishing granularity.)\n",
    "\n",
    "Semantic roles have different expression even for the same semantic meaning. We levarage LLM who is good at capturing essential semantic meaning under the cover of variance expressions. Concretely, for each semantic role expression, e.g., \"picks\", we provide groups with the predefined expressions in them, e.g. {'93': ['collect', 'collects', 'fetch', 'gather', 'gathers', 'get', 'grab', 'lift', 'pick', 'pick out', 'picks', 'select', 'take', 'take back', 'take down', 'take out', 'takes', 'takes down'], '66': ['add', 'adds', 'bottom', 'drop', 'drops', 'heap', 'heaps', 'keep', 'leave', 'let go', 'pile', 'place', 'places', 'put', 'put aside', 'put back', 'release', 'return', 'returns', 'set', 'soak', 'soaks']}. Then we let LLM to decide which group \"picks\" belongs to. \n",
    "\n",
    "Nouns (ARG1) is a bit trickier than verbs, since it seems have more variance in the expression and there could be multiple goups to go. For example, \"the bottle of spice\" could belongs to the group of \"bottle\" and the group of \"spice\" at the same time. ({'280': ['blender cup', 'bottle', 'bottle lid', 'bottle top', 'cap', 'carburator bowl', 'carburetor bowl', 'clip art lid', 'cooking pot cover', 'cover', 'cover lid', 'cover pot', 'gas tank cover', 'gear stick', 'jug lid', 'lid', 'oil filler cap', 'pen lid', 'pot cover', 'tea urn lid', 'watercolor tube top'], '473': ['spice'], '364': ['herself', 'himself', 'lady', 'man', 'person', 'shoulder', 'they', 'woman']}). But a good LLM seems can handle it situation well. Limited experiments shows that `gpt-4-1106-preview` is good enough while `gpt-3.5-turbo-1106` is not. \n",
    "\n",
    "_gpt-4-1106-preview_\n",
    "string='picks', groups={'93': ['collect', 'collects', 'fetch', 'gather', 'gathers', 'get', 'grab', 'lift', 'pick', 'pick out', 'picks', 'select', 'take', 'take back', 'take down', 'take out', 'takes', 'takes down']}\n",
    "resp={'93': True}\n",
    "string='a bag of clothes', groups={'192': ['compound', 'floor', 'ground', 'land', 'surface', 'wood floor', 'wooden floor', 'wooden floor board'], '115': ['black cloth', 'cloth', 'clothe', 'clothing material', 'cotton', 'cotton bud', 'cotton yarn', 'crochet pattern', 'cuff of shirt', 'cutting model', 'dashboard', 'fabric', 'garment', 'kanga', 'piece of cloth', 'pillow case', 'pillow cover', 'pillow garment', 'rag', 't shirt sleeve', 'table cloth', 'table cover', 'white cloth'], '12': ['bag', 'bag of manure', 'grocery', 'litter bag', 'nylon', 'nylon bag', 'nylon storage pack', 'paper bag', 'paper cover', 'paper packet', 'paperbag', 'plastic', 'plastic bag', 'polythene', 'polythene bag', 'pouch', 'sachet', 'sachet of #unsure', 'sachet of meat', 'sachet of wipe', 'sachet piece', 'sack', 'satchet', 'shopping bag', 'suitcase']}\n",
    "resp={'192': False, '115': True, '12': True}\n",
    "\n",
    "_gpt-3.5-turbo-1106_ (struggle with format)\n",
    "string='picks', groups={'93': ['collect', 'collects', 'fetch', 'gather', 'gathers', 'get', 'grab', 'lift', 'pick', 'pick out', 'picks', 'select', 'take', 'take back', 'take down', 'take out', 'takes', 'takes down']}\n",
    "resp={'groups': {'93': True}}\n",
    "e=AssertionError() retry\n",
    "resp={'1': True}\n",
    "e=AssertionError() retry\n",
    "resp={'1': True, '2': False, '3': False}\n",
    "e=AssertionError() retry\n",
    "resp={'groups': {'93': True}}\n",
    "e=AssertionError() retry\n",
    "resp={'1': True, '2': True, '3': True}\n",
    "e=AssertionError() retry\n",
    "resp={'1': True, '2': False, '3': False, '4': False, '5': False, '6': False, '7': False, '8': False, '9': False, '10': False, '11': False, '12': False, '13': False, '14': False, '15': False, '16': False}\n",
    "e=AssertionError() retry\n",
    "resp={'1': True, '2': False, '3': False}\n",
    "e=AssertionError() retry\n",
    "resp={'1': True, '2': True, '3': True}\n",
    "e=AssertionError() retry\n",
    "resp={'1': True, '2': False, '3': False}\n",
    "e=AssertionError() retry\n",
    "resp={'1': True, '2': False, '3': True}\n",
    "e=AssertionError() retry\n",
    "resp={'1': True, '2': False, '3': False}\n",
    "e=AssertionError() retry\n",
    "resp={'1': True, '2': False, '3': False}\n",
    "e=AssertionError() retry\n",
    "resp={'1': True, '2': False, '3': False}\n",
    "e=AssertionError() retry\n",
    "resp={'groups': {'93': True, '2': False, '3': False}}\n",
    "e=AssertionError() retry\n",
    "\n",
    "_gpt-4_\n",
    "e=BadRequestError('Error code: 400 - {\\'error\\': {\\'message\\': \"Invalid parameter: \\'response_format\\' of type \\'json_object\\' is not supported with this model.\", \\'type\\': \\'invalid_request_error\\', \\'param\\': \\'response_format\\', \\'code\\': None}}') retry\n",
    "\n",
    "\n",
    "Another challenge is the number of groups. When there are too many groups (categories) to be classied into, it could be financially and time wise expensive. Luckly, some annotation comes with a small set of candicate groups. They covers all groups all semantic roles in a narration could belong to. For example, narration \" C picks a bag of clothes from the floor\" has annotated verb group {'93': ['collect', 'collects', 'fetch', 'gather', 'gathers', 'get', 'grab', 'lift', 'pick', 'pick out', 'picks', 'select', 'take', 'take back', 'take down', 'take out', 'takes', 'takes down']}, and noun groups groups={'192': ['compound', 'floor', 'ground', 'land', 'surface', 'wood floor', 'wooden floor', 'wooden floor board'], '115': ['black cloth', 'cloth', 'clothe', 'clothing material', 'cotton', 'cotton bud', 'cotton yarn', 'crochet pattern', 'cuff of shirt', 'cutting model', 'dashboard', 'fabric', 'garment', 'kanga', 'piece of cloth', 'pillow case', 'pillow cover', 'pillow garment', 'rag', 't shirt sleeve', 'table cloth', 'table cover', 'white cloth'], '12': ['bag', 'bag of manure', 'grocery', 'litter bag', 'nylon', 'nylon bag', 'nylon storage pack', 'paper bag', 'paper cover', 'paper packet', 'paperbag', 'plastic', 'plastic bag', 'polythene', 'polythene bag', 'pouch', 'sachet', 'sachet of #unsure', 'sachet of meat', 'sachet of wipe', 'sachet piece', 'sack', 'satchet', 'shopping bag', 'suitcase']} which covers verb, ARG1 -- group 115 and 12, ARG2 -- group 192.\n",
    "\n",
    "For those future situations where we have to deal with large amount (hunders of) potential groups. We will discuss latter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "def string_classify(string, groups, printing=False):\n",
    "    '''\n",
    "    args:\n",
    "        string: string\n",
    "        groups: dictionary. {\"group_number\": [string1, string2, ...]}\n",
    "    '''\n",
    "    \n",
    "    if printing:\n",
    "        print(f\"{string=}, {groups=}\")\n",
    "        \n",
    "    # no need to use LLM\n",
    "    if len(groups) == 1:\n",
    "        if printing:\n",
    "            print(f\"only one group {groups.keys()}\")\n",
    "        return [int(str_gn) for str_gn in groups.keys()]\n",
    "    \n",
    "    # has to use LLM\n",
    "    client = OpenAI(api_key=\"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\")\n",
    "    while True:    \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "            model=\"gpt-4-1106-preview\",\n",
    "            response_format={ \"type\": \"json_object\" },\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert in classifying a description of objects into groups of objects. Please think step by step and be sure to choose the correst groups for the given description. Your output should be in follow JSON format where the key is group number and value is a bool indicating if the stirng is in that group or no. E.g., \\\"\\{\\\"groups\\\": true, \\\"2\\\": false, \\\"3\\\": true\\}\\\" means the description is in group 1 and group 3 but not group 2.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"The description is {string}. They groups are {groups}\"}\n",
    "            ]\n",
    "            )\n",
    "            \n",
    "            resp = json.loads(response.choices[0].message.content)\n",
    "            if printing:\n",
    "                print(f\"{resp=}\")\n",
    "            \n",
    "            group_list = []\n",
    "            for k, v in resp.items():\n",
    "                if v:\n",
    "                    assert k in groups.keys(), \"Hallucination. Got a key not in groups\"\n",
    "                    group_list.append(int(k))\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"{e=} retry\")\n",
    "            continue\n",
    "    return group_list\n",
    "    \n",
    "\n",
    "\n",
    "def determine_group(clip_row, taxonomy_noun, taxonomy_verb, printing=False):\n",
    "    tag_verb = clip_row['tag_verb'] # list\n",
    "    V = clip_row['V'] # string\n",
    "    # groups_verb = list(taxonomy_verb.iloc[tag_verb, :][\"group\"]) # list of list of strings(noun)\n",
    "    groups_verb = {f\"{index}\": row['group'] for index, row in taxonomy_verb.iloc[tag_verb, :].iterrows()}\n",
    "    group_no_verb = string_classify(V, groups_verb, printing=printing) # one of int in the tag list\n",
    "\n",
    "    tag_noun = clip_row['tag_noun'] # list\n",
    "    ARG1 = clip_row['ARG1'] # string\n",
    "    # groups_noun = list(taxonomy_0noun.iloc[tag_noun, :][\"group\"]) # list of list of strings(noun)\n",
    "    groups_noun = {f\"{index}\": row['group'] for index, row in taxonomy_noun.iloc[tag_noun, :].iterrows()}\n",
    "    \n",
    "    group_no_noun = string_classify(ARG1, groups_noun, printing=printing) # one of int in the tag list\n",
    "\n",
    "    return group_no_verb, group_no_noun\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string='puts', groups={'66': ['add', 'adds', 'bottom', 'drop', 'drops', 'heap', 'heaps', 'keep', 'leave', 'let go', 'pile', 'place', 'places', 'put', 'put aside', 'put back', 'release', 'return', 'returns', 'set', 'soak', 'soaks']}\n",
      "only one group dict_keys(['66'])\n",
      "string='the cement', groups={'90': ['cement', 'clay', 'concrete', 'concrete structure', 'mortar', 'motar']}\n",
      "only one group dict_keys(['90'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([66], [90])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the function handling one row of clip\n",
    "# determine_group(clean_srl_df.iloc[0], taxonomy_noun, taxonomy_verb)\n",
    "determine_group(clean_srl_df.iloc[random.choice(list(range(len(clean_srl_df))))], taxonomy_noun, taxonomy_verb, printing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> need faster method. one clip can cost 1.5s. Then 3M clips will be more than a month. So, even just for unique narrations (1.2M), there it will be 2 weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1164228"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique narrations\n",
    "len(clean_srl_df[\"clip_text\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1000 narrations cost 63 mins and $4 :below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_grouping = 1000\n",
    "# from tqdm.notebook import tqdm\n",
    "# tqdm.pandas(desc=\"Progress\")\n",
    "# _ = clean_srl_df.iloc[:num_grouping].progress_apply(determine_group, axis=1, args=(taxonomy_noun, taxonomy_verb))\n",
    "# _.to_csv(f\"../dataset/ego4d/annotation/grouping_gpt_output_{num_grouping}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_grouping_output = pd.read_csv(f\"../dataset/ego4d/annotation/grouping_gpt_output_{num_grouping}.csv\")\n",
    "naive_grouping_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> maybe have a stat showing how many clips are assigned into multiple groups. Sounds interestig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group_no_verb</th>\n",
       "      <th>group_no_noun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [group_no_verb, group_no_noun]\n",
       "Index: []"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_group_no_df = pd.DataFrame(naive_grouping_output[\"0\"].apply(ast.literal_eval),\n",
    "                columns=[\"group_no_verb\", \"group_no_noun\"]) # each row represent one clip row. each column represent one group_no in taxonomy df\n",
    "clip_group_no_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_no_verb_list, group_no_noun_list = zip(*naive_grouping_output[\"0\"].apply(ast.literal_eval))\n",
    " \n",
    "clip_group_no_df = pd.DataFrame({\"group_no_verb\": group_no_verb_list, \"group_no_noun\": group_no_noun_list}) # each row represent one clip row. each column represent one group_no in taxonomy df\n",
    "clip_group_no_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the \"group\" column in canonical_narrations_df as empty lists\n",
    "canonical_narrations_df['clip_uid_list'] = [[] for _ in range(len(canonical_narrations_df))]\n",
    " \n",
    "# Iterate over each row in clean_srl_df\n",
    "for index, row in clip_group_no_df.iterrows():\n",
    "    verb_no_list = row['group_no_verb']  # Get the verb numbers (list of int) for the current row\n",
    "    noun_no_list = row['group_no_noun']  # Get the noun numbers (list of int) for the current row\n",
    "    \n",
    "    # Find the corresponding canonical narration in canonical_narrations_df\n",
    "    \n",
    "    flag_v = canonical_narrations_df['verb_index'].isin(verb_no_list)\n",
    "    flag_n = canonical_narrations_df['noun_index'].isin(noun_no_list)\n",
    "    matched_canonical_narration_df = canonical_narrations_df[flag_v & flag_n]\n",
    "    \n",
    "    # Get the index of the row of the clip\n",
    "    clip_index = clean_srl_df.iloc[index]['clip_uid']\n",
    "    \n",
    "    # Update the \"group\" column of the corresponding canonical narration with the clip index\n",
    "    if len(matched_canonical_narration_df) > 0:\n",
    "        # print(f\"{matched_canonical_narration_df=}\")\n",
    "        # there could be multiple canonical narrations for one clip, add the clip_uid to all of them\n",
    "        for index, mtc_row in matched_canonical_narration_df.iterrows():\n",
    "            canonical_narrations_df.at[index, 'clip_uid_list'].append(clip_index)\n",
    "        # break\n",
    "canonical_narrations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up: only keep the rows of canonical narrations that have corresponding clips.clean_srl_df\n",
    "canonical_narrations_df_clean = canonical_narrations_df[canonical_narrations_df[\"clip_uid_list\"].apply(len) > 0]\n",
    "# canonical_narrations_df_clean = canonical_narrations_df_clean.reset_index(drop=True)\n",
    "canonical_narrations_df_clean[\"canonical_narration_text\"] = canonical_narrations_df_clean[\"canonical_verb\"] + \" \" + canonical_narrations_df_clean[\"canonical_noun\"]\n",
    "canonical_narrations_df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make validation table\n",
    "\n",
    "- copy the canonical narration except for the index of clips belongs to the canonical narration\n",
    "- For each canonical narration, random sample K clip row indexs from clips having the same canonica narration, or differnt V or diiffernt ARG1 or differnt V and ARG1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misalighmemnt_detection_benchmark = canonical_narrations_df_clean[[\"canonical_narration_text\", \"canonical_verb\", \"canonical_noun\", \"verb_index\", \"noun_index\"]]\n",
    "misalighmemnt_detection_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['002d2729-df71-438d-8396-5895b349e8fd_90.41664857606146_91.09331062393854',\n",
       " '002d2729-df71-438d-8396-5895b349e8fd_313.7813399307248_314.1196709546634']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = misalighmemnt_detection_benchmark.iloc[0]\n",
    "canonical_narrations_df.at[row.name, 'clip_uid_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misalighmemnt_detection_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_720681/1512388060.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  misalighmemnt_detection_benchmark['align'] = misalighmemnt_detection_benchmark.apply(lambda row: safe_sample(canonical_narrations_df.at[row.name, 'clip_uid_list'], k), axis=1)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "k = 5  # Number of elements to sample\n",
    "def safe_sample(lst, n):\n",
    "    if n > len(lst):\n",
    "        return lst\n",
    "    else:\n",
    "        return random.sample(lst, n)\n",
    "\n",
    "misalighmemnt_detection_benchmark['align'] = misalighmemnt_detection_benchmark.apply(lambda row: safe_sample(canonical_narrations_df.at[row.name, 'clip_uid_list'], k), axis=1)\n",
    "\n",
    "# cop: similarily, make a new column \"misalign_V\". This time, for each row, you should first find m misaligning_rows in `misalignment_detection_benchmark` that has the same `noun_index` but different `verb_index`. Then, for each misaligning_row, like I didn't above for the column `align`, randomly sample k//m element from a row of `canonical_narrations_df` in column `clip_uid`\n",
    "m = 3  # Number of misaligning rows to find\n",
    "k = 5  # Number of elements to sample per misaligning row\n",
    "\n",
    "def find_misaligning_rows(row, misalign_roles):\n",
    "    noun_index = row['noun_index']\n",
    "    verb_index = row['verb_index']\n",
    "    \n",
    "    # Find m misaligning rows with the same noun_index but different verb_index\n",
    "    if misalign_roles == ['verb']:\n",
    "        misaligning_rows_all = misalighmemnt_detection_benchmark[(misalighmemnt_detection_benchmark['noun_index'] == noun_index) & (misalighmemnt_detection_benchmark['verb_index'] != verb_index)]\n",
    "    elif misalign_roles == ['noun']:\n",
    "        misaligning_rows_all = misalighmemnt_detection_benchmark[(misalighmemnt_detection_benchmark['noun_index'] != noun_index) & (misalighmemnt_detection_benchmark['verb_index'] == verb_index)]\n",
    "    elif 'noun' in misalign_roles and 'verb' in misalign_roles:\n",
    "        misaligning_rows_all = misalighmemnt_detection_benchmark[(misalighmemnt_detection_benchmark['noun_index'] != noun_index) & (misalighmemnt_detection_benchmark['verb_index'] != verb_index)]\n",
    "    else:\n",
    "        raise ValueError(f\"misalign_roles should be ['verb'], ['noun'], or ['verb', 'noun'] but got {misalign_roles}\")\n",
    "\n",
    "    if len(misaligning_rows_all) > 0:\n",
    "        misaligning_rows = misaligning_rows_all.sample(min(m, len(misaligning_rows_all)))\n",
    "    else:\n",
    "        # If there are no misaligning rows, return an empty list\n",
    "        return []\n",
    "    \n",
    "    # Randomly sample k//m elements from each misaligning row in canonical_narrations_df\n",
    "    misalign_V = []\n",
    "    for _, misaligning_row in misaligning_rows.iterrows():\n",
    "        clip_uid_list = canonical_narrations_df.at[misaligning_row.name, 'clip_uid_list']\n",
    "        misalign_V.extend(random.sample(clip_uid_list, k//m))\n",
    "    \n",
    "    return misalign_V\n",
    "\n",
    "misalighmemnt_detection_benchmark['misalign_V'] = misalighmemnt_detection_benchmark.apply(find_misaligning_rows, args=([\"verb\"], ), axis=1)\n",
    "\n",
    "misalighmemnt_detection_benchmark['misalign_ARG1'] = misalighmemnt_detection_benchmark.apply(find_misaligning_rows, args=([\"noun\"], ), axis=1)\n",
    "\n",
    "misalighmemnt_detection_benchmark['misalign_V_ARG1'] = misalighmemnt_detection_benchmark.apply(find_misaligning_rows, args=([\"verb\", \"noun\"], ), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misalighmemnt_detection_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each row, if any of t\n",
    "# he alignness column is empty, drop the row. That means the row may not have clip rows to choose\n",
    "valid_row_bool = list(misalighmemnt_detection_benchmark[[\"align\", \"misalign_V\", \"misalign_ARG1\", \"misalign_V_ARG1\"]].apply(lambda row: sum([lt==[] for lt in row])!=0, axis=1))\n",
    "misalighmemnt_detection_benchmark = misalighmemnt_detection_benchmark.drop(misalighmemnt_detection_benchmark.index[valid_row_bool])\n",
    "misalighmemnt_detection_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aligness: \tmisalign_V_ARG1\n",
      "canonical_narration_text: \thold bowl\n",
      "verb_taxnomoy accessed by verb_index:\t hold_(support,_grip,_grasp)\n",
      "noun_taxnomoy accessed by noun_index:\t bowl\n",
      "clip_text: \tC touches her face with her left hand.\n",
      "=====================\n",
      "aligness: \tmisalign_V\n",
      "canonical_narration_text: \tmove leek\n",
      "verb_taxnomoy accessed by verb_index:\t move_(transfer,_pass,_exchange)\n",
      "noun_taxnomoy accessed by noun_index:\t leek\n",
      "clip_text: \tC takes the nylon of leeks from her left hand with her right hand.\n",
      "=====================\n",
      "aligness: \talign\n",
      "canonical_narration_text: \tremove knife\n",
      "verb_taxnomoy accessed by verb_index:\t remove\n",
      "noun_taxnomoy accessed by noun_index:\t knife_(knife,_machete)\n",
      "clip_text: \tC removes sliced vegetables from the knife in her right hand with her left hand.\n",
      "=====================\n",
      "aligness: \talign\n",
      "canonical_narration_text: \tpour bowl\n",
      "verb_taxnomoy accessed by verb_index:\t pour\n",
      "noun_taxnomoy accessed by noun_index:\t bowl\n",
      "clip_text: \tC pours the contents of the bowl into a small bucket of flour with her left hand.\n",
      "=====================\n",
      "aligness: \tmisalign_V\n",
      "canonical_narration_text: \ttouch bread\n",
      "verb_taxnomoy accessed by verb_index:\t touch\n",
      "noun_taxnomoy accessed by noun_index:\t bread_(bread,_bun,_chapati,_flatbread,_loaf,_roti,_tortilla)\n",
      "clip_text: \tC turns on the bread maker with her right hand.\n",
      "=====================\n",
      "aligness: \talign\n",
      "canonical_narration_text: \tshake chopping_board\n",
      "verb_taxnomoy accessed by verb_index:\t shake\n",
      "noun_taxnomoy accessed by noun_index:\t chopping_board\n",
      "clip_text: \tC shakes off the water on the chopping board with both hands.\n",
      "=====================\n",
      "aligness: \tmisalign_V_ARG1\n",
      "canonical_narration_text: \ttouch dough\n",
      "verb_taxnomoy accessed by verb_index:\t touch\n",
      "noun_taxnomoy accessed by noun_index:\t dough\n",
      "clip_text: \tC carries the chopping board with a knife on it from the counter top with her left hand.\n",
      "=====================\n",
      "aligness: \talign\n",
      "canonical_narration_text: \twash glass\n",
      "verb_taxnomoy accessed by verb_index:\t wash\n",
      "noun_taxnomoy accessed by noun_index:\t glass\n",
      "clip_text: \tC rinses the glass bowl under the tap with both hands.\n",
      "=====================\n",
      "aligness: \tmisalign_ARG1\n",
      "canonical_narration_text: \topen bowl\n",
      "verb_taxnomoy accessed by verb_index:\t open\n",
      "noun_taxnomoy accessed by noun_index:\t bowl\n",
      "clip_text: \tC opens the laundry machine\n",
      "=====================\n",
      "aligness: \tmisalign_V_ARG1\n",
      "canonical_narration_text: \topen sauce\n",
      "verb_taxnomoy accessed by verb_index:\t open\n",
      "noun_taxnomoy accessed by noun_index:\t sauce\n",
      "clip_text: \tC picks coriander leaves\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "# instantiate that for a row in benchmark, the columns of clip_uids are as expected\n",
    "for _i in range(10):\n",
    "    bench_row_n = random.randint(0, len(misalighmemnt_detection_benchmark))\n",
    "    aligness = random.choice([\"align\", \"misalign_V\", \"misalign_ARG1\", \"misalign_V_ARG1\"])\n",
    "    clip_uid = safe_sample(misalighmemnt_detection_benchmark.iloc[bench_row_n][aligness], 1)[0]\n",
    "    print(f\"aligness: \\t{aligness}\")\n",
    "    \n",
    "    print(\"canonical_narration_text: \\t\" + misalighmemnt_detection_benchmark.iloc[bench_row_n][\"canonical_narration_text\"])\n",
    "    print(f\"verb_taxnomoy accessed by verb_index:\\t {taxonomy_verb.iloc[misalighmemnt_detection_benchmark.iloc[bench_row_n]['verb_index']]['label']}\")\n",
    "    print(f\"noun_taxnomoy accessed by noun_index:\\t {taxonomy_noun.iloc[misalighmemnt_detection_benchmark.iloc[bench_row_n]['noun_index']]['label']}\")\n",
    "    print(\"clip_text: \\t\" + clean_srl_df[clean_srl_df[\"clip_uid\"] == clip_uid].iloc[0][\"clip_text\"])\n",
    "    print(f\"=====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clip_uid</th>\n",
       "      <th>index</th>\n",
       "      <th>video_uid</th>\n",
       "      <th>video_dur</th>\n",
       "      <th>narration_source</th>\n",
       "      <th>narration_ind</th>\n",
       "      <th>narration_time</th>\n",
       "      <th>clip_start</th>\n",
       "      <th>clip_end</th>\n",
       "      <th>clip_text</th>\n",
       "      <th>tag_verb</th>\n",
       "      <th>tag_noun</th>\n",
       "      <th>ARG0</th>\n",
       "      <th>V</th>\n",
       "      <th>ARG1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>002d2729-df71-438d-8396-5895b349e8fd_2489.6262...</td>\n",
       "      <td>1007</td>\n",
       "      <td>002d2729-df71-438d-8396-5895b349e8fd</td>\n",
       "      <td>3571.433333</td>\n",
       "      <td>narration_pass_1</td>\n",
       "      <td>804</td>\n",
       "      <td>2489.9646</td>\n",
       "      <td>2489.626267</td>\n",
       "      <td>2490.302929</td>\n",
       "      <td>C closes the plate with her left hand</td>\n",
       "      <td>[11]</td>\n",
       "      <td>[321, 236, 381]</td>\n",
       "      <td>C</td>\n",
       "      <td>closes</td>\n",
       "      <td>the plate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              clip_uid  index  \\\n",
       "899  002d2729-df71-438d-8396-5895b349e8fd_2489.6262...   1007   \n",
       "\n",
       "                                video_uid    video_dur  narration_source  \\\n",
       "899  002d2729-df71-438d-8396-5895b349e8fd  3571.433333  narration_pass_1   \n",
       "\n",
       "     narration_ind  narration_time   clip_start     clip_end  \\\n",
       "899            804       2489.9646  2489.626267  2490.302929   \n",
       "\n",
       "                                 clip_text tag_verb         tag_noun ARG0  \\\n",
       "899  C closes the plate with her left hand     [11]  [321, 236, 381]    C   \n",
       "\n",
       "          V       ARG1  \n",
       "899  closes  the plate  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some canonical narrations seems don't make sense, e.g., \"close plate\", but all canonical narrations are valid according to my design (i.e., having corresponding clips)\n",
    "clean_srl_df[clean_srl_df[\"clip_uid\"] == misalighmemnt_detection_benchmark[misalighmemnt_detection_benchmark[\"canonical_narration_text\"] == \"close plate\"].iloc[0]['align'][0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label                          plate_(dish,_plate,_platter,_saucer)\n",
       "group             [24 wall plate, 24 well plate, breadcrumb, cer...\n",
       "canonical_noun                                                plate\n",
       "Name: 381, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "label             napkin_(handkerchief,_napkin,_serviette,_tissu...\n",
       "group             [ash napkin, hand, handkerchief, kitchen tissu...\n",
       "canonical_noun                                               napkin\n",
       "Name: 321, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "label                   hand_(finger,_hand,_palm,_thumb)\n",
       "group             [finger, hand, left hand, palm, thumb]\n",
       "canonical_noun                                      hand\n",
       "Name: 236, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['24 wall plate',\n",
       " '24 well plate',\n",
       " 'breadcrumb',\n",
       " 'ceramic plate',\n",
       " 'dish',\n",
       " 'petri dish',\n",
       " 'plastic dish',\n",
       " 'plastic plate',\n",
       " 'plate',\n",
       " 'plate drainer guide',\n",
       " 'platter',\n",
       " 'porcelain',\n",
       " 'sample plate',\n",
       " 'saucer']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# palte has many meanings like here it's not the plate we eat on, but the lid on a pot (\"petri dish\" in this example)\n",
    "# This could be an error anotation or variance... That's the variance in language. kinda interesting\n",
    "display(taxonomy_noun.iloc[misalighmemnt_detection_benchmark[misalighmemnt_detection_benchmark[\"canonical_narration_text\"] == \"close plate\"].iloc[0][\"noun_index\"]])\n",
    "display(taxonomy_noun.iloc[321])\n",
    "display(taxonomy_noun.iloc[236])\n",
    "taxonomy_noun.iloc[misalighmemnt_detection_benchmark[misalighmemnt_detection_benchmark[\"canonical_narration_text\"] == \"close plate\"].iloc[0][\"noun_index\"]][\"group\"]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misalighmemnt_detection_benchmark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "misalighmemnt_detection_benchmark.to_csv(f\"../dataset/ego4d/annotation/misalignment_detection_benchmark_{num_grouping}.csv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_srl_df.to_csv(f\"../dataset/ego4d/annotation/clean_srl_df.csv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomy_verb.to_csv(f\"../dataset/ego4d/annotation/taxonomy_verb.csv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomy_noun.to_csv(f\"../dataset/ego4d/annotation/taxonomy_noun.csv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_mistake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
